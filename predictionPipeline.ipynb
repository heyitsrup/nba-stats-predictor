{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from nba_api.stats.static import players\n",
    "from nba_api.stats.endpoints import leaguegamefinder, playercareerstats\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch NBA Player Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetchPlayerData(playerName):\n",
    "    player = players.find_players_by_full_name(playerName)\n",
    "\n",
    "    if player:\n",
    "        playerID = player[0]['id']\n",
    "        careerStats = playercareerstats.PlayerCareerStats(player_id=playerID)\n",
    "        careerStatsDF = careerStats.get_data_frames()[0]\n",
    "        seasons = careerStatsDF['SEASON_ID'].unique().tolist()\n",
    "    else:\n",
    "        print(f\"Player {playerName} not found\")\n",
    "        exit()\n",
    "\n",
    "    for season in seasons:\n",
    "        gameFinder = leaguegamefinder.LeagueGameFinder(player_id_nullable=playerID, season_nullable=season)\n",
    "        desiredColumns = ['TEAM_NAME', 'GAME_DATE', 'MATCHUP','WL', 'PTS', 'REB', 'AST', 'STL', 'BLK']\n",
    "        games = games = gameFinder.get_data_frames()[0][desiredColumns]\n",
    "\n",
    "        gamesDict = games.to_dict(orient='records')\n",
    "\n",
    "        # Store data in JSON file\n",
    "        fileName = f\"data/raw/{playerName} {season} regular season games.json\"\n",
    "        with open(fileName, 'w') as JSONFile:\n",
    "            json.dump(gamesDict, JSONFile, indent=4)\n",
    "\n",
    "        print(f\"Game data stored in {fileName}\")\n",
    "\n",
    "playerNameInput = input(\"Enter player name:\")\n",
    "fetchPlayerData(playerName=playerNameInput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSONDataset(Dataset):\n",
    "    def __init__(self, JSONFile, transform=None):\n",
    "        with open(JSONFile, 'r', encoding='utf-8') as f:\n",
    "            self.data = json.load(f)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if not self.data:\n",
    "            raise IndexError(\"Dataset is empty or not loaded properly.\")\n",
    "        \n",
    "        sample = self.data[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customTransform(sample):\n",
    "    inputData = torch.tensor([\n",
    "        sample['PTS'], \n",
    "        sample['REB'], \n",
    "        sample['AST'], \n",
    "        sample['STL'], \n",
    "        sample['BLK']\n",
    "    ], dtype=torch.float32)\n",
    "    \n",
    "    target = inputData.clone()\n",
    "    \n",
    "    return inputData, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getJSONFilePaths(directory):\n",
    "    JSONFilePaths = []\n",
    "    for root, dir, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                relPath = os.path.join(root, file)\n",
    "                JSONFilePaths.append(os.path.normpath(relPath))\n",
    "    return JSONFilePaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSONFilePaths = getJSONFilePaths(\"./data/raw\")\n",
    "print(\"Relative JSON file paths:\")\n",
    "print(JSONFilePaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDatasets = [JSONDataset(json_file, transform=customTransform) for json_file in JSONFilePaths[0:-2]]\n",
    "trainLoaders = [DataLoader(dataset, batch_size=2, shuffle=True) for dataset in trainDatasets]\n",
    "\n",
    "valDataset = JSONDataset(JSONFilePaths[-2], transform=customTransform)\n",
    "valLoader = DataLoader(valDataset, batch_size=2, shuffle=False)\n",
    "\n",
    "testDataset = JSONDataset(JSONFilePaths[-1], transform=customTransform)\n",
    "testLoader = DataLoader(testDataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, inputSize=5, hiddenSize=128, outputSize=5, numLayers=1):\n",
    "        super().__init__()\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.numLayers= 1\n",
    "        self.lstm = nn.LSTM(inputSize, hiddenSize, numLayers, batch_first=True)\n",
    "        self.fc = nn.Linear(hiddenSize, outputSize)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.numLayers, x.size(0), self.hiddenSize).to(x.device)\n",
    "        c0 = torch.zeros(self.numLayers, x.size(0), self.hiddenSize).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        return self.fc(out[:, -1, :])\n",
    "\n",
    "model = LSTMModel().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel(loader, model, criterion):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        totalLoss = 0\n",
    "        for inputs, targets in loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            totalLoss += loss.item()\n",
    "    return totalLoss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(NUM_EPOCHS = 40, lr = 0.001):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.95), weight_decay=0)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        print(f'==> Epoch {epoch+1}')\n",
    "        \n",
    "        for loader in trainLoaders:\n",
    "            for inputs, targets in tqdm(loader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                inputs = inputs.unsqueeze(1)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        lossVal = testModel(valLoader, model, criterion)\n",
    "        print(f'Validation Loss: {lossVal:.3f}')\n",
    "\n",
    "trainModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "trueLabels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in testLoader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        inputs = inputs.unsqueeze(1) \n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        targetsNp = targets.cpu().numpy()\n",
    "        outputsNp = outputs.cpu().numpy()\n",
    "        \n",
    "        trueLabels.extend(targetsNp)\n",
    "        predictions.extend(outputsNp)\n",
    "\n",
    "predictionsNp = np.array(predictions)\n",
    "trueLabelsNp = np.array(trueLabels)\n",
    "\n",
    "mse = mean_squared_error(trueLabelsNp, predictionsNp)\n",
    "mae = mean_absolute_error(trueLabelsNp, predictionsNp)\n",
    "r2 = r2_score(trueLabelsNp, predictionsNp)\n",
    "\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"R-squared (R2):\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Predicted vs Actual Season Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i, metric in enumerate(['PTS', 'REB', 'AST', 'STL', 'BLK']):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(trueLabelsNp[:, i], label='Actual', marker='o')\n",
    "    plt.plot(np.maximum(np.round(predictionsNp[:, i]), 0), label='Predicted', marker='x')\n",
    "    plt.title(f'{metric}: Predicted vs Actual')\n",
    "    plt.xlabel('Game')\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict future game statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JSONToNumpy(json_file):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    metricsList = []\n",
    "    \n",
    "    for game in data:\n",
    "        metrics = [\n",
    "            game['PTS'],\n",
    "            game['REB'],\n",
    "            game['AST'],\n",
    "            game['STL'],\n",
    "            game['BLK']\n",
    "        ]\n",
    "        metricsList.append(metrics)\n",
    "    \n",
    "    metricsArray = np.array(metricsList)\n",
    "    \n",
    "    return metricsArray\n",
    "\n",
    "historicalData = JSONToNumpy(JSONFilePaths[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictNextGame(model, historicalData):\n",
    "    model.eval()\n",
    "    \n",
    "    avg_metrics = np.mean(historicalData, axis=0)\n",
    "    \n",
    "    input_data = torch.tensor(avg_metrics, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_data)\n",
    "        predictions = outputs.cpu().numpy()\n",
    "    \n",
    "    return np.maximum(np.round(predictions.flatten()), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedMetrics = predictNextGame(model, historicalData)\n",
    "print(f\"Predicted Metrics for Next Game based on Historical Data: { predictedMetrics } \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
